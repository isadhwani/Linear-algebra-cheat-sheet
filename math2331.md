# Processes and Formulas

- **Projection of $\vec{x}$ onto a Line $l$.**
  1. Choose any point on the line to be the basis $\vec{w}$ of the span.
  2. $proj_l(\vec{x}) = \frac{\vec{x} \cdot \vec{w}}{\vec{w} \cdot \vec{w}}(\vec{w})$
- **Reflection of $\vec{x}$ across a Line $l$.**
  - $2proj_l(\vec{x}) - \vec{x}$
- **Orthogonal Projection onto Subspace with Orthonormal Basis.**
  - $(\vec{x} \cdot \vec{u_1})(\vec{u_1}) + ... + (\vec{x} \cdot \vec{u_n})(\vec{u_n})$
- **Gram-Schmidt.**
  1. Normalize $\vec{V_1}$.
  2. $\vec{V_i}^\perp = \vec{V_i} - [(\vec{v_i} \cdot \vec{u_1})(\vec{u_1}) + (\vec{v_i} \cdot \vec{u_2})(\vec{u_2}) + ... + (\vec{v_i} \cdot \vec{u_{i - 1}})(\vec{u_{i - 1}})]$
  3. Normalize $\vec{V_i}.$
- **QR Factorization.**
  - $A$ is an $m \times n$ matrix. 
  - $Q$ is an $m \times n$ matrix whose columns are the result
      of the Gram-Schmidt process on $A$.
  - $R$ is an $n \times n$ upper triangular matrix abiding by the following rules:
    - $r_{(i, i)} = \lvert \lvert \vec{V_i}^\perp \rvert \rvert$
      - $r_{(1, 1)} = \lvert \lvert \vec{V_1} \rvert \rvert$
    - $r_{(i, j)} = (\vec{u_i} \cdot \vec{v_j}), i < j$
- **Associated Matrix of Orthogonal Projection.**
  - $A = QR \rightarrow P = QQ^T$
- **Least Squares Regression.**
  1. Construct $A$ with coefficients of $c_0$ (1) as first column and
     coefficients of $c_1$ ($t$) as second column.
  4. Construct $\vec{b}$ where each entry is a value of $f(t)$.
  5. Compute $A^TA$ and $A^T\vec{b}$.
  6. Solve the augmented matrix $A^TA | A^T\vec{b}$.
- **Least Squares Error.**
  - $\vec{x^*}$ is the solution.
  - $\lvert \lvert\vec{b} - A\vec{x^*} \rvert \rvert$
- **Determinants.**
  - $det(\begin{matrix} a & b \\ c & d \\\end{matrix}) = ad-bc$
  - **Co-factor Expansion.**
    1. Choose the simplest row or column.
    2. For each entry in the chosen row, compute $c_{(i, j)}(-1)^{i + j}\lvert A_{(i, j)} \rvert$.
        - $\lvert A_{(i, j)} \rvert$ is all the entries that \ aren't included in the row or column of $c_{(i, j)}$.
    3. Add every $c_{(i, j)}(-1)^{i + j}\lvert A_{(i, j)} \rvert$.
- **Eigenvalues.**
  - $det(A - \lambda I_n)$
  - For a $2\times 2$ matrix: $F_A(\lambda) = \lambda^2 - tr(A)\lambda + det(A)$
- **Eigenvectors.**
  - $E_\lambda = ker(A - \lambda I_n)$
- **Singular Value Decomposition.**
    - Goal: for any $n \times m$ matrix $A$,
      - find $m \times m$ orthogonal matrix $U$
      - find $n \times n$ orthogonal matrix $V$
      - find $m \times n$ diagonal matrix
      - $A = U\Sigma V^T$
    1. Orthogonally diagonalize $A^TA$.
    2. Find singular values $\sigma_i$ (square roots of eigenvalues).\ List in decreasing order.
    3. Make $\Sigma$ diagonal matrix using $\sigma$ as diagonal entries.
    4. Make $V$ where columns are orthonormal eigenbasis for $A^TA$.
    5. Make $U$ where $\vec{u_i} = \frac{1}{\sigma_i}A\vec{v_i}$.

# Properties and Theorems
- **Subspaces.**
    $w \subseteq R^n \iff$
  - $\vec{0} \in w$
  - $\vec{v}, \vec{u} \in w \rightarrow \vec{v} + \vec{u} \in w$
  - $\vec{v} \in w \rightarrow k\vec{v} \in w$
  - $w$ can be written as an image or kernel
- **Rank-Nullity.** $A$ is $m \times n$, $rank(A) = r$.
  - $dim(im(A)) + dim(ker(A)) = n$
  - $dim(im(A)) = r$
  - $dim(im(A)^\perp) = m - r$
  - $dim(ker(A)) = n - r$
  - $dim(ker(A)^\perp) = r$
- **Orthogonal Matrices.**
  - $A\vec{x}$ preserves length, angle between vectors, and dot product.
  - Columns of $A$ are an orthonormal basis of R^n$.
  - $A^T = A^{-1}$
- **Tranposes.**
  - $A = A^T \iff A$ is symmetric.
  - $AA^T$ is symmetric.
  - $(A + B)^T = A^T + B^T$
  - $(AB)^T = B^TA^T$
  - $rank(A) = rank(A^T)$
- **Determinants.**
  - $A$ is invertible if and only if $det(A) \neq 0$.
  - $det(A) = $ the product of $A$'s eigenvalues. 
  - $A$ is $n \times n$: $det(kA) = k^ndet(A)$
  - $det(AB) = det(A) \cdot det(B)$
  - $det(A) = det(A^T)$
  - $det(A^{-1}) = \frac{1}{det(A)}$
  - Interchange two rows: change sign of determinant.
  - Multiply row by nonzero constant $k$: multiply determinant by $k$.
  - Add a multiple of one row to another: does not change determinant.
  - If $\geq 2$ rows are equal to each other: determinant is zero.
- **Diagonalization.**
  - $dim(E_\lambda) = gemu(\lambda)$
  - $n\times n$ matrix $A$ is diagonalizable $\iff$ the sum of all the
      geometric multiplicities of all eigenvalues of $A$ is $n$.
    - $A$ is diagonalizable if and only if there are
       $n$ linearly independent eigenvectors.
